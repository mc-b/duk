{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23be5d99",
   "metadata": {},
   "source": [
    "# Rook und Ceph\n",
    "\n",
    "**Ceph** ist ein verteiltes, fehlertolerantes und hochskalierbares Speichersystem, das Objektspeicher, Blockspeicher (RBD) und verteilte Dateisysteme (CephFS) bereitstellt. Es nutzt das **CRUSH-Algorithmus**, um Daten effizient und ohne zentralen Flaschenhals auf Cluster-Knoten zu verteilen. Ceph wird häufig in Cloud- und Virtualisierungsumgebungen wie OpenStack oder Kubernetes eingesetzt.  \n",
    "\n",
    "**Rook** ist ein **Cloud-Native Storage Orchestrator**, der Ceph in Kubernetes-Clustern automatisiert bereitstellt, verwaltet und überwacht. Rook vereinfacht die Bereitstellung und Verwaltung von Ceph, indem es als Operator in Kubernetes fungiert und automatisch Storage-Pools, Monitore und OSDs (Object Storage Daemons) verwaltet.  \n",
    "\n",
    "**Vorteile von Rook und Ceph:**  \n",
    "- **Skalierbarer, verteilter Speicher** für Block-, Datei- und Objektdaten  \n",
    "- **Automatisches Self-Healing** und Fehlertoleranz  \n",
    "- **Nahtlose Kubernetes-Integration** für Stateful-Workloads  \n",
    "- **Open-Source & Software-Defined Storage**, unabhängig von proprietärer Hardware  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2401f09c",
   "metadata": {},
   "source": [
    "### rbd Modul\n",
    "\n",
    "Das **rbd-Modul** des Linux-Kernels ist ein Treiber für das **RADOS Block Device (RBD)**, das in **Ceph**-Clustern verwendet wird. Es ermöglicht den Zugriff auf verteilte, skalierbare Speichergeräte, die über das Ceph-Cluster-Storage-System bereitgestellt werden. Das rbd-Modul stellt diese Speicher als reguläre Blockgeräte im System zur Verfügung, sodass sie wie lokale Festplatten verwendet werden können.  \n",
    "\n",
    "Zu den Hauptfunktionen gehören:  \n",
    "- Direkte Anbindung an Ceph-Cluster über das Netzwerk  \n",
    "- Unterstützung für Snapshots und Klone  \n",
    "- Dynamische Skalierung und hohe Verfügbarkeit  \n",
    "- Effiziente Lastverteilung durch verteilten Speicher  \n",
    "\n",
    "Das rbd-Modul wird typischerweise in Cloud-Umgebungen und Virtualisierungslösungen wie **OpenStack** oder **Kubernetes** genutzt, um hochverfügbaren und redundanten Speicher bereitzustellen.\n",
    "\n",
    "---\n",
    "\n",
    "Bevor wir rook-ceph enablen, muss rdb im Linux Kernel aktiviert sein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476db0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get install ceph-common -y\n",
    "lsmod | grep ceph\n",
    "sudo modprobe ceph\n",
    "\n",
    "sudo modprobe rbd\n",
    "lsmod | grep rbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e748237",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone --single-branch --branch v1.16.3 https://github.com/rook/rook.git\n",
    "cd rook/deploy/examples\n",
    "kubectl create -f crds.yaml -f common.yaml -f operator.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac17d6",
   "metadata": {},
   "source": [
    "Dann können wir rook-ceph enablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c433958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods -n rook-ceph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c0253",
   "metadata": {},
   "source": [
    "### Host Storage Cluster\n",
    "\n",
    "Um rook-ceph auszuprobieren, verwenden wir die Variante \"Host Storage Cluster\".\n",
    "\n",
    "In einem \"Host Storage Cluster\" konfiguriert Rook Ceph so, dass Daten direkt auf dem Host gespeichert werden. \n",
    "\n",
    "Der `allowMultiplePerNode: true` sollte `false` sein, damit die Daten über die drei Hosts verteilt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23790de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kubectl create -f - <<EOF\n",
    "#################################################################################################################\n",
    "# Define the settings for the rook-ceph cluster with common settings for a small test cluster.\n",
    "# All nodes with available raw devices will be used for the Ceph cluster. One node is sufficient\n",
    "# in this example.\n",
    "\n",
    "# For example, to create the cluster:\n",
    "#   kubectl create -f crds.yaml -f common.yaml -f operator.yaml\n",
    "#   kubectl create -f cluster-test.yaml\n",
    "#################################################################################################################\n",
    "apiVersion: ceph.rook.io/v1\n",
    "kind: CephCluster\n",
    "metadata:\n",
    "  name: my-cluster\n",
    "  namespace: rook-ceph # namespace:cluster\n",
    "spec:\n",
    "  dataDirHostPath: /var/lib/rook\n",
    "  cephVersion:\n",
    "    image: quay.io/ceph/ceph:v19\n",
    "    allowUnsupported: true\n",
    "  mon:\n",
    "    count: 1\n",
    "    allowMultiplePerNode: true\n",
    "  mgr:\n",
    "    count: 1\n",
    "    allowMultiplePerNode: true\n",
    "    modules:\n",
    "      - name: rook\n",
    "        enabled: true\n",
    "  dashboard:\n",
    "    enabled: true\n",
    "  crashCollector:\n",
    "    disable: true\n",
    "  storage:\n",
    "    useAllNodes: true\n",
    "    useAllDevices: true\n",
    "    allowDeviceClassUpdate: true\n",
    "    allowOsdCrushWeightUpdate: false\n",
    "    #deviceFilter:\n",
    "    #config:\n",
    "    #  deviceClass: testclass\n",
    "  monitoring:\n",
    "    enabled: false\n",
    "  healthCheck:\n",
    "    daemonHealth:\n",
    "      mon:\n",
    "        interval: 45s\n",
    "        timeout: 600s\n",
    "  priorityClassNames:\n",
    "    all: system-node-critical\n",
    "    mgr: system-cluster-critical\n",
    "  disruptionManagement:\n",
    "    managePodBudgets: true\n",
    "  cephConfig:\n",
    "    global:\n",
    "      osd_pool_default_size: \"1\"\n",
    "      mon_warn_on_pool_no_redundancy: \"false\"\n",
    "      bdev_flock_retry: \"20\"\n",
    "      bluefs_buffered_io: \"false\"\n",
    "      mon_data_avail_warn: \"10\"\n",
    "---\n",
    "apiVersion: ceph.rook.io/v1\n",
    "kind: CephBlockPool\n",
    "metadata:\n",
    "  name: builtin-mgr\n",
    "  namespace: rook-ceph # namespace:cluster\n",
    "spec:\n",
    "  name: .mgr\n",
    "  replicated:\n",
    "    size: 1\n",
    "    requireSafeReplicaSize: false\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ddc964",
   "metadata": {},
   "source": [
    "Dieser Eintrag triggert den Rook Operator welcher die weiteren Storage Driver aktiviert.\n",
    "\n",
    "Am Schluss muss HEALTH_OK erscheinen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188aa2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "kubectl get pods --namespace rook-ceph\n",
    "kubectl -n rook-ceph get CephCluster,cephfilesystems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbacdc6f",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "* [Host Storage Cluster](https://rook.io/docs/rook/v1.11/CRDs/Cluster/host-cluster/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0c85d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
